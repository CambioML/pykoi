{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "\n",
    "# print(two_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyngrok\n",
    "# !pip install mlflow\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Config (Once per machine)\n",
    "reference: https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "\n",
    "```\n",
    "(pykoi) $ accelerate config\n",
    "----------------------------------------------------------------------------------In which compute environment are you running?\n",
    "This machine                                                                      \n",
    "----------------------------------------------------------------------------------Which type of machine are you using?                                              \n",
    "multi-CPU                                                                         \n",
    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1                                                                          \n",
    "Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:No                                                                          \n",
    "Do you wish to optimize your script with torch dynamo?[yes/NO]:No                 \n",
    "How many CPU(s) should be used for distributed training? [1]:8                    \n",
    "----------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "fp16                                                                              \n",
    "accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pykoi.rlhf import RLHFConfig\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import mlflow\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykoi.chat.db.qa_database import QuestionAnswerDatabase\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (Adafactor, AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, logging,\n",
    "                          pipeline, set_seed)\n",
    "from transformers.utils import PushToHubMixin\n",
    "from trl import (AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer,\n",
    "                 SFTTrainer)\n",
    "from trl.core import LengthSampler\n",
    "from trl.trainer.utils import ConstantLengthDataset, PeftSavingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CSV_HEADER_ID = 'ID'\n",
    "QA_CSV_HEADER_QUESTION = 'Question'\n",
    "QA_CSV_HEADER_ANSWER = 'Answer'\n",
    "QA_CSV_HEADER_VOTE_STATUS = 'Vote Status'\n",
    "QA_CSV_HEADER_TIMESTAMPS = 'Timestamp'\n",
    "QA_CSV_HEADER = (\n",
    "    QA_CSV_HEADER_ID,\n",
    "    QA_CSV_HEADER_QUESTION,\n",
    "    QA_CSV_HEADER_ANSWER,\n",
    "    QA_CSV_HEADER_VOTE_STATUS,\n",
    "    QA_CSV_HEADER_TIMESTAMPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL(Trainer):\n",
    "    def __init__(self, rlhf_config: RLHFConfig):\n",
    "        self._rlhf_config = rlhf_config\n",
    "        self.accelerator = Accelerator()\n",
    "        self.num_proc = self._rlhf_config.num_workers if not self._rlhf_config.streaming else None\n",
    "        set_seed(rlhf_config.seed) ## TODO: how to set seed properly in __init__?\n",
    "\n",
    "        self.ppo_config=PPOConfig(\n",
    "            steps=self._rlhf_config.total_ppo_epochs,\n",
    "            model_name=self._rlhf_config.base_model_path,\n",
    "            learning_rate=self._rlhf_config.learning_rate,\n",
    "            batch_size=self._rlhf_config.ppo_batch_size,\n",
    "            mini_batch_size=self._rlhf_config.mini_batch_size,\n",
    "            gradient_accumulation_steps=self._rlhf_config.gradient_accumulation_steps,\n",
    "            optimize_cuda_cache=True,\n",
    "            early_stopping=self._rlhf_config.early_stopping,\n",
    "            target_kl=self._rlhf_config.target_kl,\n",
    "            ppo_epochs=self._rlhf_config.ppo_epochs,\n",
    "            seed=self._rlhf_config.seed,\n",
    "            init_kl_coef=self._rlhf_config.init_kl_coef,\n",
    "            adap_kl_ctrl=self._rlhf_config.adap_kl_ctrl,\n",
    "            # accelerator_kwargs=self._rlhf_config.accelerator_kwargs,\n",
    "            )\n",
    "        \n",
    "        ## Load the base model and tokenizer and define the PPO Trainer for RL\n",
    "        self.base_tokenizer = self.create_tokenizer(rlhf_config.base_model_path)\n",
    "        self.base_dataset=self.create_dataset(self.base_tokenizer)\n",
    "        self.base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            rlhf_config.base_model_path,\n",
    "            load_in_8bit=rlhf_config.load_in_8bit,\n",
    "            # is_loaded_in_8bit = True, # TODO TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'is_loaded_in_8bit'\n",
    "            # torch_dtype=torch.float16, \n",
    "            device_map={\"\": Accelerator().local_process_index},\n",
    "            peft_config=rlhf_config.lora_config_rl, \n",
    "        )\n",
    "        self.ppo_trainer = PPOTrainer(\n",
    "            config=self.ppo_config,\n",
    "            model=self.base_model,\n",
    "            ref_model=None,\n",
    "            tokenizer=self.base_tokenizer,\n",
    "            dataset=self.base_dataset,\n",
    "            data_collator=self.data_collator,\n",
    "            # optimizer=optimizer,\n",
    "            # peft_config=lora_config, ## PPOTrainer doesn't support parameter peft_config\n",
    "        )\n",
    "        self.base_kwargs = {\n",
    "            # \"min_length\": -1,\n",
    "            \"top_k\": rlhf_config.top_k,\n",
    "            \"top_p\": rlhf_config.top_p,\n",
    "            \"do_sample\": rlhf_config.do_sample,\n",
    "            \"pad_token_id\": self.base_tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": rlhf_config.eos_token_id,\n",
    "            \"max_length\": rlhf_config.output_max_length\n",
    "        }\n",
    "\n",
    "        ## Load the reward model and tokenizer and define the reward pipeline\n",
    "        self.reward_tokenizer = self.create_tokenizer(rlhf_config.reward_model_path)\n",
    "        self.reward_dataset=self.create_dataset(self.reward_tokenizer)\n",
    "        self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rlhf_config.reward_model_path, \n",
    "            num_labels=1,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().local_process_index}\n",
    "        )\n",
    "        self.reward_kwargs = {\n",
    "            \"return_all_scores\": True,\n",
    "            \"function_to_apply\": \"none\",\n",
    "            \"batch_size\": self._rlhf_config.ppo_batch_size,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": self._rlhf_config.output_max_length\n",
    "        }\n",
    "        self.reward_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=self.reward_model,\n",
    "            # device_map={\"\": Accelerator().local_process_index},\n",
    "            # model_kwargs={\"load_in_8bit\": True},\n",
    "            model_kwargs=self.reward_kwargs,\n",
    "            tokenizer=self.reward_tokenizer,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "    def create_tokenizer(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    def data_collator(self, data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "    def create_dataset(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "        customize this function to train the model on its own dataset.\n",
    "        \"\"\"\n",
    "        args = self._rlhf_config\n",
    "        if args.dataset_type == \"local_db\":\n",
    "            qa_database = QuestionAnswerDatabase()\n",
    "            my_data_pd = qa_database.retrieve_all_question_answers_as_pandas()\n",
    "            my_data_pd = my_data_pd[my_data_pd[QA_CSV_HEADER_VOTE_STATUS]==\"up\"]\n",
    "            my_data_pd = my_data_pd[[QA_CSV_HEADER_ID,\n",
    "                                     QA_CSV_HEADER_QUESTION,\n",
    "                                     QA_CSV_HEADER_ANSWER]]\n",
    "            print(\"My local database has {} samples\".format(my_data_pd.shape[0]))\n",
    "            dataset = Dataset.from_dict(my_data_pd)\n",
    "        elif args.dataset_type == \"local_csv\":\n",
    "            dataset = load_dataset('csv', data_files=args.dataset_name)\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        elif args.dataset_type == \"huggingface\":\n",
    "            dataset = load_dataset(\n",
    "                args.dataset_name,\n",
    "                data_dir=args.dataset_subset_sft,\n",
    "                split=args.split,\n",
    "                use_auth_token=True,\n",
    "                num_proc=self.num_proc,\n",
    "                streaming=args.streaming,\n",
    "            )\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No (supported) data files or dataset script found {args.dataset_type}\")\n",
    "        \n",
    "        # dataset = dataset.train_test_split(test_size=args.train_test_split_ratio, \n",
    "        #                                    seed=args.seed)\n",
    "        # print(f\"Size of the train set: {len(dataset['train'])}. \\\n",
    "        #       Size of the validation set: {len(dataset['test'])}\")\n",
    "        \n",
    "        # dataset = dataset.select(range(self._rlhf_config.dataset_subset_rl_train))\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            queries = [\"Question: \" + q + \"\\n\\nAnswer: \" for q in examples[QA_CSV_HEADER_QUESTION]]\n",
    "            input_ids = [tokenizer(q, truncation=True)[\"input_ids\"] for q in queries]\n",
    "            return {\"query\": queries, \"input_ids\": input_ids}\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=24, ## TODO self.num_proc,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < self._rlhf_config.max_seq_length, \n",
    "                       batched=False)\n",
    "        dataset.set_format(type=\"torch\") ## TODO\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _train(self):\n",
    "        for epoch, batch in tqdm(enumerate(self.ppo_trainer.dataloader)):\n",
    "            if epoch >= self._rlhf_config.total_ppo_epochs:\n",
    "                break\n",
    "            ## embed the questions and responses to tensors\n",
    "            question_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = self.ppo_trainer.generate(\n",
    "                question_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=LengthSampler(self._rlhf_config.output_min_length, \n",
    "                                             self._rlhf_config.output_max_length),\n",
    "                **self.base_kwargs,\n",
    "            )\n",
    "            batch[QA_CSV_HEADER_ANSWER] = self.base_tokenizer.batch_decode(response_tensors, \n",
    "                                                                 skip_special_tokens=True)\n",
    "            # compute rewards and run PPO\n",
    "            texts = [q + r for q, r in zip(batch[\"query\"], batch[QA_CSV_HEADER_ANSWER])]\n",
    "            pipe_outputs = self.reward_pipe(texts, **self.reward_kwargs)\n",
    "            rewards = [torch.tensor(output[0][\"score\"] - self._rlhf_config.reward_baseline) \\\n",
    "                       for output in pipe_outputs]\n",
    "            stats = self.ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "            self.ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "            ## save weights\n",
    "            if self._rlhf_config.save_freq and epoch and \\\n",
    "                epoch % self._rlhf_config.save_freq == 0:\n",
    "                self.ppo_trainer.save_pretrained(\n",
    "                    os.path.join(self._rlhf_config.output_dir, f\"rlhf_rl_step_{epoch}\"))\n",
    "                \n",
    "    def train(self, num_processes=1):\n",
    "        notebook_launcher(self._train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mlflow experiment name.\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://x.x.x.x:5000\")\n",
    "experiment = \"rlhf_step3_rl\"\n",
    "current_time = str(datetime.datetime.now())\n",
    "mlflow_experiment_name = '/'.join([experiment, current_time])\n",
    "\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    print(\"No mlflow run in progress\")\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pykoi parameters.\n",
    "base_model_path = \"elinas/llama-7b-hf-transformers-4.29\"\n",
    "dataset_type = \"local_db\"\n",
    "reward_model_path = \"models/rlhf_step2_rw\"\n",
    "save_freq = 100\n",
    "ppo_batch_size = 32\n",
    "ppo_epochs = 4\n",
    "total_epochs = 5\n",
    "total_ppo_epochs = 5\n",
    "trained_model_path = \"./models/rlhf_step3_rl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/rlhf_step3_rl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually log pykoi parameters into mlflow. Torch level parameters are automatically logged.\n",
    "mlflow.log_param(\"pykoi_base_model_path\", base_model_path)\n",
    "mlflow.log_param(\"pykoi_dataset_type\", dataset_type)\n",
    "mlflow.log_param(\"pykoi_reward_model_path\", reward_model_path)\n",
    "mlflow.log_param(\"pykoi_save_freq\", save_freq)\n",
    "mlflow.log_param(\"pykoi_ppo_batch_size\", ppo_batch_size)\n",
    "mlflow.log_param(\"pykoi_ppo_epochs\", ppo_epochs)\n",
    "mlflow.log_param(\"pykoi_total_epochs\", total_epochs)\n",
    "mlflow.log_param(\"pykoi_total_ppo_epochs\", total_ppo_epochs)\n",
    "mlflow.log_param(\"pykoi_trained_model_path\", trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RLHFConfig.__init__() got an unexpected keyword argument 'total_ppo_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training metrics are automatically logged into mlflow.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m config \u001b[39m=\u001b[39m RLHFConfig(base_model_path\u001b[39m=\u001b[39;49mbase_model_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                           dataset_type\u001b[39m=\u001b[39;49mdataset_type,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                           reward_model_path\u001b[39m=\u001b[39;49mreward_model_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                           save_freq\u001b[39m=\u001b[39;49msave_freq,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                           ppo_batch_size\u001b[39m=\u001b[39;49mppo_batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                           ppo_epochs\u001b[39m=\u001b[39;49mppo_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                           total_epochs\u001b[39m=\u001b[39;49mtotal_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                           total_ppo_epochs\u001b[39m=\u001b[39;49mtotal_ppo_epochs\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                           )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m rlhf_step3_rl \u001b[39m=\u001b[39m RL(config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_rl_with_mlflow.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m rlhf_step3_rl\u001b[39m.\u001b[39mtrain(trained_model_path, num_processes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: RLHFConfig.__init__() got an unexpected keyword argument 'total_ppo_epochs'"
     ]
    }
   ],
   "source": [
    "# Training metrics are automatically logged into mlflow.\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "config = RLHFConfig(base_model_path=base_model_path,\n",
    "                          dataset_type=dataset_type,\n",
    "                          reward_model_path=reward_model_path,\n",
    "                          save_freq=save_freq,\n",
    "                          ppo_batch_size=ppo_batch_size,\n",
    "                          ppo_epochs=ppo_epochs,\n",
    "                          total_epochs=total_epochs,\n",
    "                          total_ppo_epochs=total_ppo_epochs\n",
    "                          )\n",
    "rlhf_step3_rl = RL(config)\n",
    "rlhf_step3_rl.train(trained_model_path, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model into mlflow artifacts.\n",
    "mlflow.log_artifacts(trained_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal, run\n",
    "```\n",
    "mlflow ui\n",
    "```\n",
    "and go to http://127.0.0.1:5000 in the browser to view the experiment in the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
