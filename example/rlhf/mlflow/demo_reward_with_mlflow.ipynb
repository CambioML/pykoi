{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "\n",
    "# print(two_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyngrok\n",
    "# !pip install mlflow\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pykoi.rlhf import RLHFConfig\n",
    "from pykoi.rlhf import SupervisedFinetuning\n",
    "import mlflow\n",
    "import datetime\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RLHF using the data from database\n",
    "\n",
    "Let's take a look of the QA data and process it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is InstructGPT?</td>\n",
       "      <td>InstructGPT is a language model developed by O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does InstructGPT work?</td>\n",
       "      <td>InstructGPT works due to a two-step training p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some commonly used evaluation metrics...</td>\n",
       "      <td>One main evaluation metric for InstructGPT is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is InstructGPT used?</td>\n",
       "      <td>InstructGPT can be used in any application tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some common applications of InstructGPT?</td>\n",
       "      <td>Common applications of InstructGPT can be in e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                               What is InstructGPT?   \n",
       "1                         Why does InstructGPT work?   \n",
       "2  What are some commonly used evaluation metrics...   \n",
       "3                           How is InstructGPT used?   \n",
       "4  What are some common applications of InstructGPT?   \n",
       "\n",
       "                                              Answer  \n",
       "0  InstructGPT is a language model developed by O...  \n",
       "1  InstructGPT works due to a two-step training p...  \n",
       "2  One main evaluation metric for InstructGPT is ...  \n",
       "3  InstructGPT can be used in any application tha...  \n",
       "4  Common applications of InstructGPT can be in e...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_root = \"input/\"\n",
    "input_file = \"rlhf_qa_dataset.json\"\n",
    "stack_exchange_df = pd.read_json(input_root + input_file)\n",
    "stack_exchange_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My ranking database has 45 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question', 'Answer'],\n",
       "    num_rows: 45\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "print(\"My ranking database has {} samples\".format(stack_exchange_df.shape[0]))\n",
    "dataset = Dataset.from_dict(stack_exchange_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/05 01:22:16 INFO mlflow.tracking.fluent: Experiment with name 'rlhf_step2_reward/2023-10-05 01:22:16.267979' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/ubuntu/pykoi/example/rlhf/mlflow/mlruns/529075251471367022', creation_time=1696468936281, experiment_id='529075251471367022', last_update_time=1696468936281, lifecycle_stage='active', name='rlhf_step2_reward/2023-10-05 01:22:16.267979', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up mlflow experiment name.\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://x.x.x.x:5000\")\n",
    "experiment = \"rlhf_step2_reward\"\n",
    "current_time = str(datetime.datetime.now())\n",
    "mlflow_experiment_name = '/'.join([experiment, current_time])\n",
    "\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    print(\"No mlflow run in progress\")\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pykoi parameters.\n",
    "base_model_path = \"elinas/llama-7b-hf-transformers-4.29\"\n",
    "dataset_type = \"local_db\"\n",
    "trained_model_path = \"./models/rlhf_step2_rw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/rlhf_step2_rw'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually log pykoi parameters into mlflow. Torch level parameters are automatically logged.\n",
    "mlflow.log_param(\"pykoi_base_model_path\", base_model_path)\n",
    "mlflow.log_param(\"pykoi_dataset_type\", dataset_type)\n",
    "mlflow.log_param(\"pykoi_trained_model_path\", trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My local database has 126 up vote samples for SFT\n",
      "Size of the train set: 113.               Size of the validation set: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 4.71 GiB already allocated; 85.06 MiB free; 4.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run supervised finetuning\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Training metrics are automatically logged into mlflow.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m RLHFConfig(base_model_path\u001b[39m=\u001b[39mbase_model_path, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                           dataset_type\u001b[39m=\u001b[39mdataset_type,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                           \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                           )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m rlhf_step1_sft \u001b[39m=\u001b[39m SupervisedFinetuning(config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.213.36.238/home/ubuntu/pykoi/example/rlhf/mlflow/demo_reward_with_mlflow.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m rlhf_step1_sft\u001b[39m.\u001b[39mtrain_and_save(trained_model_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/pykoi/rlhf/supervised_finetuning.py:95\u001b[0m, in \u001b[0;36mSupervisedFinetuning.__init__\u001b[0;34m(self, rlhf_config, enable_telemetry)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m# self.torch_dtype = torch.bfloat16 if bf16 else (torch.float16 if fp16 else torch.float32)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     75\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39moutput_dir,\n\u001b[1;32m     76\u001b[0m     dataloader_drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     ddp_find_unused_parameters\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     94\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rlhf_config\u001b[39m.\u001b[39;49mbase_model_path,\n\u001b[1;32m     97\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rlhf_config\u001b[39m.\u001b[39;49mload_in_8bit,\n\u001b[1;32m     98\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rlhf_config\u001b[39m.\u001b[39;49mdevice_map,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m    101\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m    102\u001b[0m     args\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     packing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/modeling_utils.py:2903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2894\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2896\u001b[0m     (\n\u001b[1;32m   2897\u001b[0m         model,\n\u001b[1;32m   2898\u001b[0m         missing_keys,\n\u001b[1;32m   2899\u001b[0m         unexpected_keys,\n\u001b[1;32m   2900\u001b[0m         mismatched_keys,\n\u001b[1;32m   2901\u001b[0m         offload_index,\n\u001b[1;32m   2902\u001b[0m         error_msgs,\n\u001b[0;32m-> 2903\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2904\u001b[0m         model,\n\u001b[1;32m   2905\u001b[0m         state_dict,\n\u001b[1;32m   2906\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2907\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2908\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2909\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2910\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2911\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2912\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2913\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2914\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2915\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2916\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2917\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2918\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2919\u001b[0m     )\n\u001b[1;32m   2921\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2922\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/modeling_utils.py:3260\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3250\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3251\u001b[0m     state_dict,\n\u001b[1;32m   3252\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3256\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3257\u001b[0m )\n\u001b[1;32m   3259\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3260\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3261\u001b[0m         model_to_load,\n\u001b[1;32m   3262\u001b[0m         state_dict,\n\u001b[1;32m   3263\u001b[0m         loaded_keys,\n\u001b[1;32m   3264\u001b[0m         start_prefix,\n\u001b[1;32m   3265\u001b[0m         expected_keys,\n\u001b[1;32m   3266\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3267\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3268\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3269\u001b[0m         state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3270\u001b[0m         state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3271\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3272\u001b[0m         is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3273\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3274\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3275\u001b[0m     )\n\u001b[1;32m   3276\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3277\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/modeling_utils.py:725\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    722\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    724\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 725\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    726\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    727\u001b[0m             )\n\u001b[1;32m    729\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/utils/bitsandbytes.py:97\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     95\u001b[0m kwargs \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m is_8bit:\n\u001b[0;32m---> 97\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mInt8Params(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     98\u001b[0m \u001b[39melif\u001b[39;00m is_4bit:\n\u001b[1;32m     99\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:306\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\n\u001b[1;32m    298\u001b[0m     \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    302\u001b[0m     device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m ):\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     new_param \u001b[39m=\u001b[39m Int8Params(\n\u001b[1;32m    309\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\n\u001b[1;32m    310\u001b[0m             device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype, non_blocking\u001b[39m=\u001b[39mnon_blocking\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         has_fp16_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/koi/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:269\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcuda(device)\n\u001b[1;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[39m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mhalf()\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    270\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdouble_quant(B)\n\u001b[1;32m    271\u001b[0m     \u001b[39mdel\u001b[39;00m CBt\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 4.71 GiB already allocated; 85.06 MiB free; 4.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Run supervised finetuning\n",
    "# Training metrics are automatically logged into mlflow.\n",
    "config = RLHFConfig(base_model_path=base_model_path, \n",
    "                          dataset_type=dataset_type,\n",
    "                          \n",
    "                          )\n",
    "rlhf_step1_sft = SupervisedFinetuning(config)\n",
    "rlhf_step1_sft.train_and_save(trained_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained reward model and input into mlflow artifacts.\n",
    "mlflow.log_artifacts(trained_model_path)\n",
    "mlflow.log_artifacts(input_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
