{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "\n",
    "# print(two_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Config (Once per machine)\n",
    "reference: https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "\n",
    "```\n",
    "(pykoi) $ accelerate config\n",
    "----------------------------------------------------------------------------------In which compute environment are you running?\n",
    "This machine                                                                      \n",
    "----------------------------------------------------------------------------------Which type of machine are you using?                                              \n",
    "multi-CPU                                                                         \n",
    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1                                                                          \n",
    "Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:No                                                                          \n",
    "Do you wish to optimize your script with torch dynamo?[yes/NO]:No                 \n",
    "How many CPU(s) should be used for distributed training? [1]:8                    \n",
    "----------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "fp16                                                                              \n",
    "accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pykoi.rlhf import RLHFConfig\n",
    "\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykoi.chat.db.qa_database import QuestionAnswerDatabase\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (Adafactor, AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, logging,\n",
    "                          pipeline, set_seed)\n",
    "from transformers.utils import PushToHubMixin\n",
    "from trl import (AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer,\n",
    "                 SFTTrainer)\n",
    "from trl.core import LengthSampler\n",
    "from trl.trainer.utils import ConstantLengthDataset, PeftSavingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CSV_HEADER_ID = 'ID'\n",
    "QA_CSV_HEADER_QUESTION = 'Question'\n",
    "QA_CSV_HEADER_ANSWER = 'Answer'\n",
    "QA_CSV_HEADER_VOTE_STATUS = 'Vote Status'\n",
    "QA_CSV_HEADER_TIMESTAMPS = 'Timestamp'\n",
    "QA_CSV_HEADER = (\n",
    "    QA_CSV_HEADER_ID,\n",
    "    QA_CSV_HEADER_QUESTION,\n",
    "    QA_CSV_HEADER_ANSWER,\n",
    "    QA_CSV_HEADER_VOTE_STATUS,\n",
    "    QA_CSV_HEADER_TIMESTAMPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL(Trainer):\n",
    "    def __init__(self, rlhf_config: RLHFConfig):\n",
    "        self._rlhf_config = rlhf_config\n",
    "        self.accelerator = Accelerator()\n",
    "        self.num_proc = self._rlhf_config.num_workers if not self._rlhf_config.streaming else None\n",
    "        set_seed(rlhf_config.seed) ## TODO: how to set seed properly in __init__?\n",
    "\n",
    "        self.ppo_config=PPOConfig(\n",
    "            steps=self._rlhf_config.total_ppo_epochs,\n",
    "            model_name=self._rlhf_config.base_model_path,\n",
    "            learning_rate=self._rlhf_config.learning_rate,\n",
    "            batch_size=self._rlhf_config.ppo_batch_size,\n",
    "            mini_batch_size=self._rlhf_config.mini_batch_size,\n",
    "            gradient_accumulation_steps=self._rlhf_config.gradient_accumulation_steps,\n",
    "            optimize_cuda_cache=True,\n",
    "            early_stopping=self._rlhf_config.early_stopping,\n",
    "            target_kl=self._rlhf_config.target_kl,\n",
    "            ppo_epochs=self._rlhf_config.ppo_epochs,\n",
    "            seed=self._rlhf_config.seed,\n",
    "            init_kl_coef=self._rlhf_config.init_kl_coef,\n",
    "            adap_kl_ctrl=self._rlhf_config.adap_kl_ctrl,\n",
    "            # accelerator_kwargs=self._rlhf_config.accelerator_kwargs,\n",
    "            )\n",
    "        \n",
    "        ## Load the base model and tokenizer and define the PPO Trainer for RL\n",
    "        self.base_tokenizer = self.create_tokenizer(rlhf_config.base_model_path)\n",
    "        self.base_dataset=self.create_dataset(self.base_tokenizer)\n",
    "        self.base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            rlhf_config.base_model_path,\n",
    "            load_in_8bit=rlhf_config.load_in_8bit,\n",
    "            # is_loaded_in_8bit = True, # TODO TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'is_loaded_in_8bit'\n",
    "            # torch_dtype=torch.float16, \n",
    "            device_map={\"\": Accelerator().local_process_index},\n",
    "            peft_config=rlhf_config.lora_config_rl, \n",
    "        )\n",
    "        self.ppo_trainer = PPOTrainer(\n",
    "            config=self.ppo_config,\n",
    "            model=self.base_model,\n",
    "            ref_model=None,\n",
    "            tokenizer=self.base_tokenizer,\n",
    "            dataset=self.base_dataset,\n",
    "            data_collator=self.data_collator,\n",
    "            # optimizer=optimizer,\n",
    "            # peft_config=lora_config, ## PPOTrainer doesn't support parameter peft_config\n",
    "        )\n",
    "        self.base_kwargs = {\n",
    "            # \"min_length\": -1,\n",
    "            \"top_k\": rlhf_config.top_k,\n",
    "            \"top_p\": rlhf_config.top_p,\n",
    "            \"do_sample\": rlhf_config.do_sample,\n",
    "            \"pad_token_id\": self.base_tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": rlhf_config.eos_token_id,\n",
    "            \"max_length\": rlhf_config.output_max_length\n",
    "        }\n",
    "\n",
    "        ## Load the reward model and tokenizer and define the reward pipeline\n",
    "        self.reward_tokenizer = self.create_tokenizer(rlhf_config.reward_model_path)\n",
    "        self.reward_dataset=self.create_dataset(self.reward_tokenizer)\n",
    "        self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rlhf_config.reward_model_path, \n",
    "            num_labels=1,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().local_process_index}\n",
    "        )\n",
    "        self.reward_kwargs = {\n",
    "            \"return_all_scores\": True,\n",
    "            \"function_to_apply\": \"none\",\n",
    "            \"batch_size\": self._rlhf_config.ppo_batch_size,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": self._rlhf_config.output_max_length\n",
    "        }\n",
    "        self.reward_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=self.reward_model,\n",
    "            # device_map={\"\": Accelerator().local_process_index},\n",
    "            # model_kwargs={\"load_in_8bit\": True},\n",
    "            model_kwargs=self.reward_kwargs,\n",
    "            tokenizer=self.reward_tokenizer,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "    def create_tokenizer(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    def data_collator(self, data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "    def create_dataset(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "        customize this function to train the model on its own dataset.\n",
    "        \"\"\"\n",
    "        args = self._rlhf_config\n",
    "        if args.dataset_type == \"local_db\":\n",
    "            qa_database = QuestionAnswerDatabase()\n",
    "            my_data_pd = qa_database.retrieve_all_question_answers_as_pandas()\n",
    "            my_data_pd = my_data_pd[my_data_pd[QA_CSV_HEADER_VOTE_STATUS]==\"up\"]\n",
    "            my_data_pd = my_data_pd[[QA_CSV_HEADER_ID,\n",
    "                                     QA_CSV_HEADER_QUESTION,\n",
    "                                     QA_CSV_HEADER_ANSWER]]\n",
    "            print(\"My local database has {} samples\".format(my_data_pd.shape[0]))\n",
    "            dataset = Dataset.from_dict(my_data_pd)\n",
    "        elif args.dataset_type == \"local_csv\":\n",
    "            dataset = load_dataset('csv', data_files=args.dataset_name)\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        elif args.dataset_type == \"huggingface\":\n",
    "            dataset = load_dataset(\n",
    "                args.dataset_name,\n",
    "                data_dir=args.dataset_subset_sft,\n",
    "                split=args.split,\n",
    "                use_auth_token=True,\n",
    "                num_proc=self.num_proc,\n",
    "                streaming=args.streaming,\n",
    "            )\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No (supported) data files or dataset script found {args.dataset_type}\")\n",
    "        \n",
    "        # dataset = dataset.train_test_split(test_size=args.train_test_split_ratio, \n",
    "        #                                    seed=args.seed)\n",
    "        # print(f\"Size of the train set: {len(dataset['train'])}. \\\n",
    "        #       Size of the validation set: {len(dataset['test'])}\")\n",
    "        \n",
    "        # dataset = dataset.select(range(self._rlhf_config.dataset_subset_rl_train))\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            queries = [\"Question: \" + q + \"\\n\\nAnswer: \" for q in examples[QA_CSV_HEADER_QUESTION]]\n",
    "            input_ids = [tokenizer(q, truncation=True)[\"input_ids\"] for q in queries]\n",
    "            return {\"query\": queries, \"input_ids\": input_ids}\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=24, ## TODO self.num_proc,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < self._rlhf_config.max_seq_length, \n",
    "                       batched=False)\n",
    "        dataset.set_format(type=\"torch\") ## TODO\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _train(self):\n",
    "        for epoch, batch in tqdm(enumerate(self.ppo_trainer.dataloader)):\n",
    "            if epoch >= self._rlhf_config.total_ppo_epochs:\n",
    "                break\n",
    "            ## embed the questions and responses to tensors\n",
    "            question_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = self.ppo_trainer.generate(\n",
    "                question_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=LengthSampler(self._rlhf_config.output_min_length, \n",
    "                                             self._rlhf_config.output_max_length),\n",
    "                **self.base_kwargs,\n",
    "            )\n",
    "            batch[QA_CSV_HEADER_ANSWER] = self.base_tokenizer.batch_decode(response_tensors, \n",
    "                                                                 skip_special_tokens=True)\n",
    "            # compute rewards and run PPO\n",
    "            texts = [q + r for q, r in zip(batch[\"query\"], batch[QA_CSV_HEADER_ANSWER])]\n",
    "            pipe_outputs = self.reward_pipe(texts, **self.reward_kwargs)\n",
    "            rewards = [torch.tensor(output[0][\"score\"] - self._rlhf_config.reward_baseline) \\\n",
    "                       for output in pipe_outputs]\n",
    "            stats = self.ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "            self.ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "            ## save weights\n",
    "            if self._rlhf_config.save_freq and epoch and \\\n",
    "                epoch % self._rlhf_config.save_freq == 0:\n",
    "                self.ppo_trainer.save_pretrained(\n",
    "                    os.path.join(self._rlhf_config.output_dir, f\"rlhf_rl_step_{epoch}\"))\n",
    "                \n",
    "    def train(self, num_processes=1):\n",
    "        notebook_launcher(self._train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RLHFConfig' object has no attribute 'total_ppo_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m RLHFConfig(base_model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39melinas/llama-7b-hf-transformers-4.29\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m# \"elinas/llama-7b-hf-transformers-4.29\", \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                           dataset_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal_db\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                           reward_model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgoldmermaid/rlhf_reward_model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                           )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m rlhf_step3_rl \u001b[39m=\u001b[39m RL(config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m rlhf_step3_rl\u001b[39m.\u001b[39mtrain(\u001b[39m\"\u001b[39m\u001b[39m./models/rlhf_step3_rl\u001b[39m\u001b[39m\"\u001b[39m, num_processes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_proc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mnum_workers \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mstreaming \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m set_seed(rlhf_config\u001b[39m.\u001b[39mseed) \u001b[39m## TODO: how to set seed properly in __init__?\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mppo_config\u001b[39m=\u001b[39mPPOConfig(\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rlhf_config\u001b[39m.\u001b[39;49mtotal_ppo_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mbase_model_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mlearning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mppo_batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     mini_batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mmini_batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     optimize_cuda_cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     early_stopping\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mearly_stopping,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     target_kl\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mtarget_kl,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     ppo_epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mppo_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     init_kl_coef\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39minit_kl_coef,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     adap_kl_ctrl\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlhf_config\u001b[39m.\u001b[39madap_kl_ctrl,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# accelerator_kwargs=self._rlhf_config.accelerator_kwargs,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m## Load the base model and tokenizer and define the PPO Trainer for RL\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.215.248.75/home/ubuntu/pykoi/example/rlhf/demo_rl.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_tokenizer(rlhf_config\u001b[39m.\u001b[39mbase_model_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RLHFConfig' object has no attribute 'total_ppo_epochs'"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "config = RLHFConfig(base_model_path=\"elinas/llama-7b-hf-transformers-4.29\", # \"elinas/llama-7b-hf-transformers-4.29\", \n",
    "                          dataset_type=\"local_db\",\n",
    "                          reward_model_path=\"goldmermaid/rlhf_reward_model\",\n",
    "                          save_freq=100,\n",
    "                          ppo_batch_size=32,\n",
    "                          ppo_epochs=1,\n",
    "\n",
    "                          )\n",
    "rlhf_step3_rl = RL(config)\n",
    "rlhf_step3_rl.train(\"./models/rlhf_step3_rl\", num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
